%%%%%%%%%%%%%%%%%
%% TFGs y TFMs %%
%%%%%%%%%%%%%%%%%

@mastersthesis{tfg_javi,
  author = {Javier de la Peña},
  title = {Sistema de visión estereo aplicado a la detección y seguimiento de objetos en conducción autonoma},
  school = {UAH Politécnica},
  year = {2021},
  type = {Trabajo de Fin de Grado},
  url = {http://hdl.handle.net/10017/49468},
}

@mastersthesis{tfg_miguel,
  author = {Miguel Antunes},
  title = {Sistema de visión estereo aplicado a la detección y seguimiento de objetos en conducción autonoma},
  school = {UAH Politécnica},
  year = {2021},
  type = {Trabajo de Fin de Grado},
  url = {http://hdl.handle.net/10017/49487},
}

%%%%%%%%%%%%%%%
%% Web pages %%
%%%%%%%%%%%%%%%

@techreport{autonomous_vehicles_1,
  title = {AUTONOMOUS VEHICLES FACTSHEET},
  url = {https://css.umich.edu/factsheets/autonomous-vehicles-factsheet},
  orgatization = {Center for Sustainable Systems, University of Michigan},
  year = {2021}
}

@techreport{adas_spain,
  title = {Sistemas ADAS: qué son, cómo funcionan y cuáles son obligatorios en 2022},
  url = {https://www.businessinsider.es/sistemas-adas-son-como-funcionan-cuales-son-obligatorios-2022-1010823},
  orgatization = {Business Insider España},
  year = {2022}
}

@techreport{adas_eu,
  title = {Los asistentes a la conducción que serán obligatorios en Europa para 2022},
  url = {https://www.carwow.es/blog/sistemas-adas-obligatorios-europa-2022},
  orgatization = {Carwow},
  year = {2021}
}

@techreport{autonomy_levels,
  title = {The 6 Levels of Vehicle Autonomy Explained},
  url = {https://www.synopsys.com/automotive/autonomous-driving-levels.html},
  organization = {Velodyne Lidar},
}

@techreport{tesla_waymo,
  title = {WAYMO VS TESLA: WHAT ARE THE DIFFERENCES?},
  url = {https://www.motorverso.com/waymo-vs-tesla/},
  organization = {Motor Verso},
  year = {2022}
}

@techreport{advantages_camera,
  title = {RADAR, LiDAR and Cameras Technologies for ADAS and Autonomous Vehicles},
  url = {https://www.onelectrontech.com/radar-lidar-and-cameras-technologies-for-adas-and-autonomous-vehicles/},
  organization = {OnElectronTech},
  year = {2019}
}

@techreport{what_lidar,
  title = {What is lidar?},
  url = {https://oceanservice.noaa.gov/facts/lidar.html},
  organization = {National Ocean Service},
}

@techreport{what_fusion,
  title = {What is Sensor Fusion?},
  url = {https://appen.com/blog/what-is-sensor-fusion/},
  organization = {Appen},
}

@techreport{what_ml,
  title = {Machine Learning},
  url = {https://www.ibm.com/cloud/learn/machine-learning},
  organization = {IBM},
  year = {2020}
}

@techreport{ml_techs,
  title = {Top 6 Machine Learning Techniques},
  url = {https://analyticssteps.com/blogs/top-6-machine-learning-techniques},
  organization = {analyticSteps},
  year = {2021}
}

@techreport{what_dl,
  title = {Deep Learning},
  url = {https://www.ibm.com/cloud/learn/deep-learning},
  organization = {IBM},
  year = {2020}
}

@techreport{what_nn,
  title = {Neural Networks},
  url = {https://www.ibm.com/cloud/learn/neural-networks},
  organization = {IBM},
  year = {2020}
}

@techreport{adv_disafv_dl,
  title = {Advantages and Disadvantages of Deep Learning},
  url = {https://www.newsmaritime.com/2021/advantages-disadvantages-deep-learning/},
  organization = {Maritime News},
  year = {2021}
}

%%%%%%%%%%%%%
%% Article %%
%%%%%%%%%%%%%

@Article{paper_felipe,
AUTHOR = {Arango, J. Felipe and Bergasa, Luis M. and Revenga, Pedro A. and Barea, Rafael and López-Guillén, Elena and Gómez-Huélamo, Carlos and Araluce, Javier and Gutiérrez, Rodrigo},
TITLE = {Drive-By-Wire Development Process Based on ROS for an Autonomous Electric Vehicle},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6121},
URL = {https://www.mdpi.com/1424-8220/20/21/6121},
ISSN = {1424-8220},
ABSTRACT = {This paper presents the development process of a robust and ROS-based Drive-By-Wire system designed for an autonomous electric vehicle from scratch over an open source chassis. A revision of the vehicle characteristics and the different modules of our navigation architecture is carried out to put in context our Drive-by-Wire system. The system is composed of a Steer-By-Wire module and a Throttle-By-Wire module that allow driving the vehicle by using some commands of lineal speed and curvature, which are sent through a local network from the control unit of the vehicle. Additionally, a Manual/Automatic switching system has been implemented, which allows the driver to activate the autonomous driving and safely taking control of the vehicle at any time. Finally, some validation tests were performed for our Drive-By-Wire system, as a part of our whole autonomous navigation architecture, showing the good working of our proposal. The results prove that the Drive-By-Wire system has the behaviour and necessary requirements to automate an electric vehicle. In addition, after 812 h of testing, it was proven that it is a robust Drive-By-Wire system, with high reliability. The developed system is the basis for the validation and implementation of new autonomous navigation techniques developed within the group in a real vehicle.},
DOI = {10.3390/s20216121}
}

@Article{paper_object_detection,
AUTHOR = {Khatab, Esraa and Onsy, Ahmed and Abouelfarag, Ahmed},
TITLE = {Evaluation of 3D Vulnerable Objects; Detection Using a Multi-Sensors System for Autonomous Vehicles},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {1663},
URL = {https://www.mdpi.com/1424-8220/22/4/1663},
PubMedID = {35214569},
ISSN = {1424-8220},
ABSTRACT = {One of the primary tasks undertaken by autonomous vehicles (AVs) is object detection, which comes ahead of object tracking, trajectory estimation, and collision avoidance. Vulnerable road objects (e.g., pedestrians, cyclists, etc.) pose a greater challenge to the reliability of object detection operations due to their continuously changing behavior. The majority of commercially available AVs, and research into them, depends on employing expensive sensors. However, this hinders the development of further research on the operations of AVs. In this paper, therefore, we focus on the use of a lower-cost single-beam LiDAR in addition to a monocular camera to achieve multiple 3D vulnerable object detection in real driving scenarios, all the while maintaining real-time performance. This research also addresses the problems faced during object detection, such as the complex interaction between objects where occlusion and truncation occur, and the dynamic changes in the perspective and scale of bounding boxes. The video-processing module works upon a deep-learning detector (YOLOv3), while the LiDAR measurements are pre-processed and grouped into clusters. The output of the proposed system is objects classification and localization by having bounding boxes accompanied by a third depth dimension acquired by the LiDAR. Real-time tests show that the system can efficiently detect the 3D location of vulnerable objects in real-time scenarios.},
DOI = {10.3390/s22041663}
}

@Article{paper_comparison_lidar_camera,
AUTHOR = {Mugunthan M and Balaji SB and Harini C and Naresh. V. H and Prasannaa Venkatesh V},
TITLE = {Comparison Review on LiDAR vs Camera in Autonomous Vehicle},
JOURNAL = {IRJET},
VOLUME = {07},
YEAR = {2020},
URL = {https://www.irjet.net/archives/V7/i8/IRJET-V7I8731.pdf},
ABSTRACT = {Innovations in automation make life simpler. In the auto sector, the newly designed and planned cars require sensors to get the data and analyze it to make decisions. Tesla and Waymo have racked up the most miles and are trying to produce an autonomous vehicle without using LiDAR (light detection and ranging). Cameras are subject to the issue of change in lighting conditions.one viable solution to LiDAR versus camera debate for autonomous cars is to combining the technology. Sensor fusion in the cars fuses the data from the surrounding environment and from the sensors like radar and LiDAR to augment their perception. Undoubtedly, sensors used by self-driving vehicles will continue to progress and move past human perception. While computer vision can power self-driving cars, other sensors will keep going to the perception stack to aid with increased environmental sensing over time. This paper presents a comparison review of using LiDAR, camera and sensor fusion in autonomous cars.)},
}

@misc{GTP3,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{AlphaFold,
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  author = {John Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Zidek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A. A. Kohl and Andrew J. Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
  title = {Highly accurate protein structure prediction with AlphaFold},
  publisher = {nature},
  year = {2020},
}

@misc{MV3D,
  doi = {10.48550/ARXIV.1611.07759},
  url = {https://arxiv.org/abs/1611.07759},
  author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Multi-View 3D Object Detection Network for Autonomous Driving},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Frustum_PointNets,
  doi = {10.48550/ARXIV.1711.08488},
  url = {https://arxiv.org/abs/1711.08488},
  author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Frustum PointNets for 3D Object Detection from RGB-D Data},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointNet,
  doi = {10.48550/ARXIV.1612.00593},
  url = {https://arxiv.org/abs/1612.00593},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointNet++,
  doi = {10.48550/ARXIV.1706.02413},
  url = {https://arxiv.org/abs/1706.02413},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointPainting,
  doi = {10.48550/ARXIV.1911.10150},
  url = {https://arxiv.org/abs/1911.10150},
  author = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {PointPainting: Sequential Fusion for 3D Object Detection},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DeepLabv3+,
  doi = {10.48550/ARXIV.1706.05587},
  url = {https://arxiv.org/abs/1706.05587},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointPillars,
  doi = {10.48550/ARXIV.1812.05784},
  url = {https://arxiv.org/abs/1812.05784},
  author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointPillars: Fast Encoders for Object Detection from Point Clouds},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%
%% TFGs y TFMs %%
%%%%%%%%%%%%%%%%%

@mastersthesis{tfg_javi,
  author = {Javier de la Peña},
  title = {Algoritmos de detección de objetos 3D basados en LiDAR: comparación entre técnicas PCL clásicas y Deep Learning},
  school = {UAH Politécnica},
  year = {2021},
  type = {Trabajo de Fin de Grado},
  url = {http://hdl.handle.net/10017/49468},
}

@mastersthesis{tfg_miguel,
  author = {Miguel Antunes},
  title = {Sistema de visión estéreo aplicado a la detección y seguimiento de objetos en conducción autónoma},
  school = {UAH Politécnica},
  year = {2021},
  type = {Trabajo de Fin de Grado},
  url = {http://hdl.handle.net/10017/49487},
}

%%%%%%%%%%%%%%%
%% Web pages %%
%%%%%%%%%%%%%%%

@techreport{autonomous_vehicles_1,
  title = {AUTONOMOUS VEHICLES FACTSHEET},
  url = {https://css.umich.edu/factsheets/autonomous-vehicles-factsheet},
  orgatization = {Center for Sustainable Systems, University of Michigan},
  year = {2021}
}

@techreport{adas_spain,
  title = {Sistemas ADAS: qué son, cómo funcionan y cuáles son obligatorios en 2022},
  url = {https://www.businessinsider.es/sistemas-adas-son-como-funcionan-cuales-son-obligatorios-2022-1010823},
  orgatization = {Business Insider España},
  year = {2022}
}

@techreport{adas_eu,
  title = {Los asistentes a la conducción que serán obligatorios en Europa para 2022},
  url = {https://www.carwow.es/blog/sistemas-adas-obligatorios-europa-2022},
  orgatization = {Carwow},
  year = {2021}
}

@techreport{autonomy_levels,
  title = {The 6 Levels of Vehicle Autonomy Explained},
  url = {https://www.synopsys.com/automotive/autonomous-driving-levels.html},
  organization = {Velodyne Lidar},
}

@techreport{tesla_waymo,
  title = {WAYMO VS TESLA: WHAT ARE THE DIFFERENCES?},
  url = {https://www.motorverso.com/waymo-vs-tesla/},
  organization = {Motor Verso},
  year = {2022}
}

@techreport{advantages_camera,
  title = {RADAR, LiDAR and Cameras Technologies for ADAS and Autonomous Vehicles},
  url = {https://www.onelectrontech.com/radar-lidar-and-cameras-technologies-for-adas-and-autonomous-vehicles/},
  organization = {OnElectronTech},
  year = {2019}
}

@techreport{what_lidar,
  title = {What is lidar?},
  url = {https://oceanservice.noaa.gov/facts/lidar.html},
  organization = {National Ocean Service},
}

@techreport{what_fusion,
  title = {What is Sensor Fusion?},
  url = {https://appen.com/blog/what-is-sensor-fusion/},
  organization = {Appen},
}

@techreport{what_ml,
  title = {Machine Learning},
  url = {https://www.ibm.com/cloud/learn/machine-learning},
  organization = {IBM},
  year = {2020}
}

@techreport{ml_techs,
  title = {Top 6 Machine Learning Techniques},
  url = {https://analyticssteps.com/blogs/top-6-machine-learning-techniques},
  organization = {analyticSteps},
  year = {2021}
}

@techreport{what_dl,
  title = {Deep Learning},
  url = {https://www.ibm.com/cloud/learn/deep-learning},
  organization = {IBM},
  year = {2020}
}

@techreport{what_nn,
  title = {Neural Networks},
  url = {https://www.ibm.com/cloud/learn/neural-networks},
  organization = {IBM},
  year = {2020}
}

@techreport{adv_disafv_dl,
  title = {Advantages and Disadvantages of Deep Learning},
  url = {https://www.newsmaritime.com/2021/advantages-disadvantages-deep-learning/},
  organization = {Maritime News},
  year = {2021}
}

@misc{YOLOv5,
  title = {YOLOv5 GitHub repository},
  url = {https://github.com/ultralytics/yolov5},
  organization = {Ultralytics},
  year = {2021}
}

@techreport{what_cnn,
  title = {Convolutional Neural Networks},
  url = {https://www.ibm.com/cloud/learn/convolutional-neural-networks},
  organization = {IBM},
  year = {2020}
}

@techreport{what_tl,
  title = {What Is Transfer Learning? Exploring the Popular Deep Learning Approach.},
  url = {https://builtin.com/data-science/transfer-learning},
  organization = {builtin},
  year = {2021}
}

@techreport{top_img_datasets,
  title = {The Top Image Datasets and Their Challenges.},
  url = {https://zbigatron.com/the-top-image-datasets/},
  organization = {Zbigatron},
  year = {2018}
}

@techreport{yolo_comparison,
  title = {YOLO: Real-Time Object Detection Explained.},
  url = {https://www.v7labs.com/blog/yolo-object-detection#},
  organization = {V7Labs},
  year = {2022}
}

@misc{kitti_evaluator_3d,
  title = {3D Object Detection Evaluation 2017},
  url = {http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d},
  organization = {Karlsruhe Institute of Technology},
  year = {2022}
}

%%%%%%%%%%%%%
%% Article %%
%%%%%%%%%%%%%

@Article{paper_felipe,
AUTHOR = {Arango, J. Felipe and Bergasa, Luis M. and Revenga, Pedro A. and Barea, Rafael and López-Guillén, Elena and Gómez-Huélamo, Carlos and Araluce, Javier and Gutiérrez, Rodrigo},
TITLE = {Drive-By-Wire Development Process Based on ROS for an Autonomous Electric Vehicle},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6121},
URL = {https://www.mdpi.com/1424-8220/20/21/6121},
ISSN = {1424-8220},
ABSTRACT = {This paper presents the development process of a robust and ROS-based Drive-By-Wire system designed for an autonomous electric vehicle from scratch over an open source chassis. A revision of the vehicle characteristics and the different modules of our navigation architecture is carried out to put in context our Drive-by-Wire system. The system is composed of a Steer-By-Wire module and a Throttle-By-Wire module that allow driving the vehicle by using some commands of lineal speed and curvature, which are sent through a local network from the control unit of the vehicle. Additionally, a Manual/Automatic switching system has been implemented, which allows the driver to activate the autonomous driving and safely taking control of the vehicle at any time. Finally, some validation tests were performed for our Drive-By-Wire system, as a part of our whole autonomous navigation architecture, showing the good working of our proposal. The results prove that the Drive-By-Wire system has the behaviour and necessary requirements to automate an electric vehicle. In addition, after 812 h of testing, it was proven that it is a robust Drive-By-Wire system, with high reliability. The developed system is the basis for the validation and implementation of new autonomous navigation techniques developed within the group in a real vehicle.},
DOI = {10.3390/s20216121}
}

@Article{paper_object_detection,
AUTHOR = {Khatab, Esraa and Onsy, Ahmed and Abouelfarag, Ahmed},
TITLE = {Evaluation of 3D Vulnerable Objects; Detection Using a Multi-Sensors System for Autonomous Vehicles},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {1663},
URL = {https://www.mdpi.com/1424-8220/22/4/1663},
PubMedID = {35214569},
ISSN = {1424-8220},
ABSTRACT = {One of the primary tasks undertaken by autonomous vehicles (AVs) is object detection, which comes ahead of object tracking, trajectory estimation, and collision avoidance. Vulnerable road objects (e.g., pedestrians, cyclists, etc.) pose a greater challenge to the reliability of object detection operations due to their continuously changing behavior. The majority of commercially available AVs, and research into them, depends on employing expensive sensors. However, this hinders the development of further research on the operations of AVs. In this paper, therefore, we focus on the use of a lower-cost single-beam LiDAR in addition to a monocular camera to achieve multiple 3D vulnerable object detection in real driving scenarios, all the while maintaining real-time performance. This research also addresses the problems faced during object detection, such as the complex interaction between objects where occlusion and truncation occur, and the dynamic changes in the perspective and scale of bounding boxes. The video-processing module works upon a deep-learning detector (YOLOv3), while the LiDAR measurements are pre-processed and grouped into clusters. The output of the proposed system is objects classification and localization by having bounding boxes accompanied by a third depth dimension acquired by the LiDAR. Real-time tests show that the system can efficiently detect the 3D location of vulnerable objects in real-time scenarios.},
DOI = {10.3390/s22041663}
}

@Article{paper_comparison_lidar_camera,
AUTHOR = {Mugunthan M and Balaji SB and Harini C and Naresh. V. H and Prasannaa Venkatesh V},
TITLE = {Comparison Review on LiDAR vs Camera in Autonomous Vehicle},
JOURNAL = {IRJET},
VOLUME = {07},
YEAR = {2020},
URL = {https://www.irjet.net/archives/V7/i8/IRJET-V7I8731.pdf},
ABSTRACT = {Innovations in automation make life simpler. In the auto sector, the newly designed and planned cars require sensors to get the data and analyze it to make decisions. Tesla and Waymo have racked up the most miles and are trying to produce an autonomous vehicle without using LiDAR (light detection and ranging). Cameras are subject to the issue of change in lighting conditions.one viable solution to LiDAR versus camera debate for autonomous cars is to combining the technology. Sensor fusion in the cars fuses the data from the surrounding environment and from the sensors like radar and LiDAR to augment their perception. Undoubtedly, sensors used by self-driving vehicles will continue to progress and move past human perception. While computer vision can power self-driving cars, other sensors will keep going to the perception stack to aid with increased environmental sensing over time. This paper presents a comparison review of using LiDAR, camera and sensor fusion in autonomous cars.)},
}

@misc{GTP3,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{AlphaFold,
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  author = {John Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Zidek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A. A. Kohl and Andrew J. Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
  title = {Highly accurate protein structure prediction with AlphaFold},
  publisher = {nature},
  year = {2020},
}

@misc{MV3D,
  doi = {10.48550/ARXIV.1611.07759},
  url = {https://arxiv.org/abs/1611.07759},
  author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Multi-View 3D Object Detection Network for Autonomous Driving},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Frustum_PointNets,
  doi = {10.48550/ARXIV.1711.08488},
  url = {https://arxiv.org/abs/1711.08488},
  author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Frustum PointNets for 3D Object Detection from RGB-D Data},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointNet,
  doi = {10.48550/ARXIV.1612.00593},
  url = {https://arxiv.org/abs/1612.00593},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointNet++,
  doi = {10.48550/ARXIV.1706.02413},
  url = {https://arxiv.org/abs/1706.02413},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointPainting,
  doi = {10.48550/ARXIV.1911.10150},
  url = {https://arxiv.org/abs/1911.10150},
  author = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {PointPainting: Sequential Fusion for 3D Object Detection},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DeepLabv3+,
  doi = {10.48550/ARXIV.1706.05587},
  url = {https://arxiv.org/abs/1706.05587},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PointPillars,
  doi = {10.48550/ARXIV.1812.05784},
  url = {https://arxiv.org/abs/1812.05784},
  author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointPillars: Fast Encoders for Object Detection from Point Clouds},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SECOND,
  title={SECOND: Sparsely Embedded Convolutional Detection},
  author={Yan Yan and Yuxing Mao and Bo Li},
  journal={Sensors (Basel, Switzerland)},
  year={2018},
  volume={18}
}

@misc{PointRCNN,
  doi = {10.48550/ARXIV.1812.04244},
  url = {https://arxiv.org/abs/1812.04244},
  author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{kitti_dataset,
  author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Are we ready for autonomous driving? The KITTI vision benchmark suite}, 
  year={2012},
  volume={},
  number={},
  pages={3354-3361},
  doi={10.1109/CVPR.2012.6248074}
}

@InProceedings{nuscenes_dataset,
  author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  title = {nuScenes: A Multimodal Dataset for Autonomous Driving},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2020}
}

@InProceedings{waymo_dataset,
  author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
  title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2020}
}

@article{KITTI-360,
   title =  {{KITTI}-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D},
   author = {Yiyi Liao and Jun Xie and Andreas Geiger},
   journal = {arXiv preprint arXiv:2109.13410},
   year = {2021},
}

@inproceedings{carla,
  title = { {CARLA}: {An} Open Urban Driving Simulator},
  author = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = {1--16},
  year = {2017}
}

@incollection{Pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{Tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@misc{YOLOv4,
  doi = {10.48550/ARXIV.2004.10934},
  url = {https://arxiv.org/abs/2004.10934},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{VGG16,
  doi = {10.48550/ARXIV.1705.03004},
  url = {https://arxiv.org/abs/1705.03004},
  author = {Qassim, Hussam and Feinzimer, David and Verma, Abhishek},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Residual Squeeze VGG16},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ResNet,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{COCO_dataset,
  doi = {10.48550/ARXIV.1405.0312},
  url = {https://arxiv.org/abs/1405.0312},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Microsoft COCO: Common Objects in Context},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ImageNet_dataset,
  doi = {10.48550/ARXIV.1409.0575},
  url = {https://arxiv.org/abs/1409.0575},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.8; I.5.2},
  title = {ImageNet Large Scale Visual Recognition Challenge},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{fastai,
	doi = {10.3390/info11020108},
	url = {https://doi.org/10.3390\%2Finfo11020108},
	year = 2020,
	month = {feb},
	publisher = {{MDPI} {AG}},
	volume = {11},
	number = {2},
	pages = {108},
	author = {Jeremy Howard and Sylvain Gugger},
	title = {Fastai: A Layered {API} for Deep Learning},
	journal = {Information}
}

@misc{YOLOv3,
  doi = {10.48550/ARXIV.1804.02767},
  url = {https://arxiv.org/abs/1804.02767},
  author = {Redmon, Joseph and Farhadi, Ali},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {YOLOv3: An Incremental Improvement},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Faster_R-CNN,
  doi = {10.48550/ARXIV.1506.01497},
  url = {https://arxiv.org/abs/1506.01497},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{GooLeNet,
  doi = {10.48550/ARXIV.1409.4842},
  url = {https://arxiv.org/abs/1409.4842},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Going Deeper with Convolutions},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{EfficientDet,
  doi = {10.48550/ARXIV.1911.09070},
  url = {https://arxiv.org/abs/1911.09070},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {EfficientDet: Scalable and Efficient Object Detection},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


